import torch
import os
import streamlit as st
import logging
from model import ExercisePredictionModel, FoodPredictionModel  # ë°˜ë“œì‹œ model.pyì—ì„œ ê°€ì ¸ì˜´
from transformers import AutoTokenizer, AutoModelForCausalLM

# ê´€ë¦¬ììš© ë¡œê¹… ì„¤ì • (ì½˜ì†” ë˜ëŠ” íŒŒì¼ì— ê¸°ë¡)
logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
MODEL_EXERCISE_PATH = "models/model_exercise.pth"
MODEL_FOOD_PATH = "models/model_food.pth"

def load_model(model_path, model_class, input_dim=13):
    """
    PyTorch ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜.
    weights_only ì˜µì…˜ìœ¼ë¡œ ì‹œë„í•˜ë©°, ì‹¤íŒ¨ ì‹œ weights_only=Falseë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    """
    model = model_class(input_dim)
    if not os.path.exists(model_path):
        logging.error(f"ğŸš¨ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    try:
        checkpoint = torch.load(model_path, map_location=torch.device("cpu"), weights_only=True)
        model.load_state_dict(checkpoint, strict=False)
        model.eval()
        logging.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        return model
    except Exception as e:
        logging.error(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (weights_only=True ì‹¤íŒ¨): {e}")
        try:
            checkpoint = torch.load(model_path, map_location=torch.device("cpu"), weights_only=False)
            model.load_state_dict(checkpoint, strict=False)
            model.eval()
            logging.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ (weights_only=False): {model_path}")
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ (weights_only=False): {model_path}")
            return model
        except Exception as e2:
            logging.error(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (weights_only=False): {e2}")
    return None

# ëª¨ë¸ ë¡œë“œ ì‹¤í–‰
model_exercise = load_model(MODEL_EXERCISE_PATH, ExercisePredictionModel, input_dim=13)
model_food = load_model(MODEL_FOOD_PATH, FoodPredictionModel, input_dim=13)

# í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” secrets.tomlì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
HF_API_KEY = os.getenv("HF_API_KEY")  # ë¯¸ë¦¬ ì„ ì–¸ëœ API í‚¤ ë³€ìˆ˜

@st.cache_resource
def load_gemma_model():
    """
    Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ í•¨ìˆ˜.
    `token` ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ì¦ í† í°ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    """
    model_name = "google/gemma-2-9b-it"
    try:
        hf_token = HF_API_KEY
        if not hf_token:
            logging.error("ğŸš¨ HF_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return None, None
        Gemma_tokenizer= AutoTokenizer.from_pretrained("google/gemma-2-9b-it", token=HF_API_KEY)
        print("âœ… GemmaTokenizer ì •ìƒ ë¡œë”© ì™„ë£Œ!")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16,
            token=hf_token
        )
        logging.info("âœ… Gemma ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        print("âœ… Gemma ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return Gemma_tokenizer, model
    except Exception as e:
        logging.error(f"ğŸš¨ Gemma ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print (f"ğŸš¨ Gemma ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

# Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (UIì— ë©”ì‹œì§€ í‘œì‹œ X)
gemma_tokenizer, gemma_model = load_gemma_model()

